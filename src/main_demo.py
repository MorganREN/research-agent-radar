from sqlmodel import Session, select
from src.research_agent.storage.models import Paper, create_db_and_tables, engine
from src.research_agent.agents.scout.arxiv_scout import ArxivScout
from src.research_agent.agents.scout.elsevier_scout import ElsevierScout
from src.research_agent.agents.filter.triage_agent import RelevanceFilter
from src.research_agent.acquisition.downloader import DownloadManager
from src.research_agent.agents.analysis.reviewer import PaperReviewer
from loguru import logger
import asyncio

def run_ingestion_pipeline():
    # 1. åˆå§‹åŒ–æ•°æ®åº“
    create_db_and_tables()
    
    # 2. é…ç½®ä½ çš„ç ”ç©¶å…´è¶£ (è¿™æ˜¯é«˜åº¦å®šåˆ¶åŒ–çš„éƒ¨åˆ†)
    # ç»“åˆäº† AI å’Œ åœŸæœ¨/éš§é“å·¥ç¨‹ [cite: 244]
    my_interests = """
    1. äººå·¥æ™ºèƒ½åœ¨åœŸæœ¨å·¥ç¨‹ä¸­çš„åº”ç”¨ã€‚
    2. éš§é“å·¥ç¨‹çš„å˜å½¢é¢„æµ‹ã€ç»“æ„å¥åº·ç›‘æµ‹
    3. æ•°å­—å­ªç”ŸæŠ€æœ¯
    4. äººå·¥æ™ºèƒ½
    5. å¤§è¯­è¨€æ¨¡å‹
    6. éŸ³ä¹ç”Ÿæˆ
    7. è§†é¢‘ç”Ÿæˆæ¨¡å‹å’Œè®¡ç®—æœºè§†è§‰
    """
    
    # 3. åˆå§‹åŒ– Agents
    # 3.1 æœç´¢ arXiv çš„åœŸæœ¨å·¥ç¨‹(cs.CE) å’Œ äººå·¥æ™ºèƒ½(cs.AI) æ¿å—
    arxiv_scout = ArxivScout(query="cat:cs.CE OR cat:cs.AI", max_results=10)
    # 3.2 æœç´¢ Elsevier çš„æŒ‡å®šæœŸåˆŠ
    elsevier_scout = ElsevierScout(
        max_results=5,
        year=2026
    )

    # 4. åˆå§‹åŒ– Filter
    triage = RelevanceFilter(research_interests=my_interests)
    
    # 5. è¿è¡Œ Scout (ä¾¦å¯Ÿ)
    new_papers = []
    new_papers += arxiv_scout.fetch_papers()
    new_papers += elsevier_scout.fetch_papers()


    with Session(engine) as session:
        for paper in new_papers:
            # 5.1 å»é‡æ£€æŸ¥ (æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨)
            existing_paper = session.get(Paper, paper.id)
            if existing_paper:
                logger.info(f"â­ï¸  è·³è¿‡å·²å­˜åœ¨çš„è®ºæ–‡: {paper.id}")
                continue
            
            # 5.2 è¿è¡Œ Filter (ç­›é€‰)
            logger.info(f"ğŸ§  æ­£åœ¨åˆ†æè®ºæ–‡ç›¸å…³æ€§: {paper.title[:50]}...")
            result = triage.check_relevance(paper.title, paper.abstract)
            
            # 5.3 æ›´æ–°ç»“æœ
            paper.is_relevant = result['is_relevant']
            paper.relevance_reason = result['reason']
            
            # 5.4 å­˜å…¥æ•°æ®åº“
            session.add(paper)
            session.commit()
            
            icon = "âœ…" if paper.is_relevant else "âŒ"
            print(f"{icon} [{paper.id}] åˆ¤å®šç»“æœ: {paper.is_relevant}")
            print(f"   ç†ç”±: {paper.relevance_reason}\n")

async def run_analysis_phase():
    '''
    Docstring for run_analysis_phase
    '''
    reviewer = PaperReviewer()
    downloader = DownloadManager()

    with Session(engine) as session:
        # 1. è·å–æ‰€æœ‰å·²ä¸‹è½½ä¸”ç›¸å…³çš„è®ºæ–‡
        papers = session.exec(
            select(Paper).where(
                Paper.is_relevant == True,
            )
        ).all()

        for paper in papers:
            if paper.analysis_report:
                logger.info(f"è·³è¿‡å·²åˆ†æçš„è®ºæ–‡: {paper.id}")
                continue
            logger.info(f"å¼€å§‹åˆ†æè®ºæ–‡: {paper.id} ...")

            # 2. ç¡®è®¤ PDF å·²ä¸‹è½½
            save_path = f"data/papers/{paper.id}.pdf".replace(":", "_")
            if paper.download_status != "downloaded":
                status = await downloader.process_download(
                    paper_id=paper.id,
                    url=paper.url,
                    source=paper.source
                )
                if status != "downloaded":
                    logger.error(f"è®ºæ–‡ {paper.id} ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡åˆ†æã€‚")
                    continue
                paper.download_status = status
                session.add(paper)
                session.commit()

            # 3. è¿è¡Œåˆ†æ
            if paper.download_status == "downloaded":
                if paper.source == "arxiv":
                    report = reviewer.analyze_paper(paper, pdf_path=save_path)
                else:
                    report = reviewer.analyze_paper(paper, xml_content=paper.full_text_content)
                paper.analysis_report = report
                session.add(paper)
                session.commit()
                logger.success(f"è®ºæ–‡ {paper.id} åˆ†æå®Œæˆã€‚")


if __name__ == "__main__":
    run_ingestion_pipeline()
    
    asyncio.run(run_analysis_phase())