from src.research_agent.agents.analysis.parser import PDFParser
from src.research_agent.storage.models import Paper, engine

from sqlmodel import Session
from openai import OpenAI
import os
from dotenv import load_dotenv
from loguru import logger
import yaml
import datetime as dt

load_dotenv() # åŠ è½½ .env ä¸­çš„ API KEY

class PDFUploadParser:
    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.parser = PDFParser() # å¼•ç”¨ä¸Šé¢çš„è§£æå™¨

    def refresh_database(self, paper: Paper):
        """å°†è§£æåçš„è®ºæ–‡ä¿¡æ¯å­˜å‚¨åˆ°æ•°æ®åº“ä¸­"""
        try:
            with Session(engine) as session:
                session.add(paper)
                session.commit()
                session.refresh(paper)
                logger.info(f"âœ… è®ºæ–‡ä¿¡æ¯å·²å­˜å‚¨åˆ°æ•°æ®åº“: {paper.title}")
        except Exception as e:
            logger.error(f"æ•°æ®åº“å­˜å‚¨é”™è¯¯: {e}")

    def parse_info(self, pdf_path: str) -> dict:
        """ä» PDF ä¸­æå–å…³é”®ä¿¡æ¯ï¼ˆå¦‚æ ‡é¢˜ã€æ‘˜è¦ç­‰ï¼‰"""
        parse_prompt = '''
    You are a helpful assistant that extracts key information from academic papers. Given the content of a paper, extract the following information in JSON format:
1. title: The title of the paper
2. abstract: The abstract of the paper
3. authors: A list of authors
4. published_date: The publication date (if available)

The output should be a JSON object with the above fields. If any field is not found, return an empty string or empty list for that field.
    '''
        full_text = self.parser.parse_to_markdown(pdf_path)
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": parse_prompt},
                    {"role": "user", "content": f"è®ºæ–‡å…¨æ–‡å†…å®¹:\n{full_text[:10000]}"} # æˆªå–å‰1wå­—ç¬¦é˜²æº¢å‡º
                ]
            )
            print(f"ğŸ“‘ æå–åˆ°çš„è®ºæ–‡ä¿¡æ¯: {response.choices[0].message.content}")
            raw_reponse = response.choices[0].message.content
            # the original response may contain some explanation, we need to extract the JSON part
            try:
                json_start = raw_reponse.find("{")
                json_end = raw_reponse.rfind("}") + 1
                json_str = raw_reponse[json_start:json_end]
                basic_info = yaml.safe_load(json_str)
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•è§£æ JSONï¼Œä½¿ç”¨åŸå§‹å“åº”: {e}")
                basic_info = yaml.safe_load(raw_reponse)  # ç›´æ¥å°è¯•è§£ææ•´ä¸ªå“åº”

            paper = Paper(
                id=f"uploaded:{os.path.basename(pdf_path)}",
                title=basic_info.get("title", ""),
                abstract=basic_info.get("abstract", ""),
                authors=basic_info.get("authors", []),
                url=f"google.com/search?q={basic_info.get('title', '').replace(' ', '+')}",
                published_date=dt.datetime.today(),
                source="uploaded_pdf",
                is_relevant=True,  # é»˜è®¤ä¸Šä¼ çš„è®ºæ–‡éƒ½ç›¸å…³
                download_status="downloaded",
            )

            logger.info(f"âœ… æˆåŠŸè§£æå¹¶å­˜å‚¨è®ºæ–‡: {paper.title}")
            self.refresh_database(paper)
            return paper
        except Exception as e:
            logger.error(f"LLM ä¿¡æ¯æå–å‡ºé”™: {e}")
            return {}



if __name__ == "__main__":
    parser_upload = PDFUploadParser()
    sample_pdf = "data/papers/buildings-13-02725-v4.pdf"  # æ›¿æ¢ä¸ºå®é™…çš„ PDF æ–‡ä»¶è·¯å¾„
    print(parser_upload.parse_info(sample_pdf))